{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual CPE matching evaluation\n",
    "\n",
    "This notebook assists the manual matching evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_certs.dataset import CCDataset\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input data for label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = CCDataset.from_json(\"/Users/adam/phd/projects/certificates/sec-certs/datasets/cc_04_10_2022/cc.json\")\n",
    "df = dset.to_pandas()\n",
    "\n",
    "eval_digests = pd.read_csv(\"./../../data/cpe_eval/random.csv\").set_index(\"dgst\").index\n",
    "eval_certs = df.loc[df.index.isin(eval_digests)]\n",
    "\n",
    "# It may be handy to display max number of assigned cpe matches here\n",
    "eval_certs[\"n_cpes\"] = eval_certs.cpe_matches.map(len)\n",
    "max_n_cpes = eval_certs.n_cpes.max()\n",
    "\n",
    "# Now you may want to adjust the key `cpe_n_max_matches` config in sec_certs/config/settings.yml according to max_n_cpes\n",
    "# This helps to avoid clutter in label studio interface\n",
    "\n",
    "dset.certs = {x.dgst: x for x in dset if x.dgst in eval_certs.index.tolist()}\n",
    "dset.to_label_studio_json(\"./label_studio_input_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Now you import this data to label studio and label it`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from label studio and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/adam/Downloads/cpe_hundred.json\", \"r\") as handle:\n",
    "    data = json.load(handle)\n",
    "\n",
    "\n",
    "results = []\n",
    "for sample in data:\n",
    "    option_keys = [key for key in sample.keys() if \"option_\" in key]\n",
    "    n_cpe_matches = len([sample[key] for key in option_keys if sample[key] != \"No good match\"])\n",
    "\n",
    "    if not \"verified_cpe_match\" in sample.keys():\n",
    "        n_wrong = 0\n",
    "    elif isinstance(sample[\"verified_cpe_match\"], str):\n",
    "        n_wrong = 1\n",
    "    else:\n",
    "        n_wrong = len(sample[\"verified_cpe_match\"])\n",
    "\n",
    "    results.append([n_cpe_matches, n_wrong])\n",
    "\n",
    "    #if isinstance(sample[\"verified_cpe_match\"], str)\n",
    "\n",
    "correct = [x[0] for x in results]\n",
    "wrong = [x[1] for x in results]\n",
    "n_candidates = [x[0] + x[1] for x in results]\n",
    "completely_right = [x == 0 for x in wrong]\n",
    "\n",
    "print(f\"Evaluated {sum(n_candidates)} CPE matches in {len(results)} certificates\")\n",
    "print(f\"In total, {sum(correct)} ({(100 * sum(correct) / sum(n_candidates)):.2f}%) are correct (precision of the positive class).\")\n",
    "print(f\"Also, {sum(completely_right)} ({(100 * sum(completely_right) / len(n_candidates)):.2f}%) certificates have perfect matches.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5b8c5b127d2cfe5bc3a1c933e197485eb9eba25154c3661362401503b4ef9d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
