{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "\n",
    "# When on Aura, it is important to first set CUDA_VISIBLE_DEVICES environment variable directly from notebook\n",
    "# For available GPUs, see https://www.fi.muni.cz/tech/unix/aura.html.cs\n",
    "# Must be done before any related package that leverages cuda is imported\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-56c53afb-6f08-5e5b-83fa-32fc6f09eeb0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\n",
    "\n",
    "import pandas as pd\n",
    "from sec_certs.dataset import CCDataset\n",
    "from shutil import copy\n",
    "from pathlib import Path\n",
    "from sec_certs.model.references.segment_extractor import ReferenceSegmentExtractor\n",
    "from sec_certs.utils.nlp import prec_recall_metric\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sec_certs.utils.nlp import prec_recall_metric\n",
    "from sec_certs.model.references.annotator_trainer import ReferenceAnnotatorTrainer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sec_certs.utils.helpers import compute_heuristics_version\n",
    "from rapidfuzz import fuzz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "import optuna\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "DATASET_PATH = REPO_ROOT / \"dataset/cc_final_run_may_23/dataset.json\"\n",
    "ANNOTATIONS_PATH = REPO_ROOT / \"src/sec_certs/data/reference_annotations/manual_annotations/\"\n",
    "\n",
    "def replace_all(text: str, to_replce: set[str]) -> str:\n",
    "    for i in to_replce:\n",
    "        text = text.replace(i, \"\")\n",
    "    return text\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Enrich annotations with string similarity of cert. and referenced cert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = pd.read_csv(ANNOTATIONS_PATH / \"train.csv\")\n",
    "valid_annotations = pd.read_csv(ANNOTATIONS_PATH / \"valid.csv\")\n",
    "all_annotations = pd.concat([train_annotations, valid_annotations])\n",
    "all_annotations = all_annotations[all_annotations.label != \"None\"].assign(label=lambda df: df.label.str.upper())\n",
    "\n",
    "dset = CCDataset.from_json(DATASET_PATH)\n",
    "all_certs = {x.dgst: x for x in dset.certs.values()}\n",
    "dset.certs = {x.dgst: x for x in dset.certs.values() if x.dgst in all_annotations.dgst.unique()}\n",
    "\n",
    "cert_id_to_name_mapping = {x.heuristics.cert_id: x.name for x in all_certs.values()}\n",
    "all_annotations[\"referenced_cert_name\"] = all_annotations[\"referenced_cert_id\"].map(cert_id_to_name_mapping)\n",
    "all_annotations[\"cert_name\"] = all_annotations[\"dgst\"].map(lambda x: dset[x].name)\n",
    "all_annotations[\"cert_versions\"] = all_annotations[\"cert_name\"].map(compute_heuristics_version)\n",
    "all_annotations = all_annotations.loc[all_annotations[\"referenced_cert_name\"].notnull()].copy()\n",
    "all_annotations[\"referenced_cert_versions\"] = all_annotations[\"referenced_cert_name\"].map(compute_heuristics_version)\n",
    "all_annotations[\"cert_name_stripped_version\"] = all_annotations.apply(lambda x: replace_all(x[\"cert_name\"], x[\"cert_versions\"]), axis=1)\n",
    "all_annotations[\"referenced_cert_name_stripped_version\"] = all_annotations.apply(lambda x: replace_all(x[\"referenced_cert_name\"], x[\"referenced_cert_versions\"]), axis=1)\n",
    "all_annotations[\"name_similarity\"] = all_annotations.apply(lambda x: fuzz.token_set_ratio(x[\"cert_name\"], x[\"referenced_cert_name\"]), axis=1)\n",
    "all_annotations[\"name_similarity_stripped_version\"] = all_annotations.apply(lambda x: fuzz.token_set_ratio(x[\"cert_name_stripped_version\"], x[\"referenced_cert_name_stripped_version\"]), axis=1)\n",
    "all_annotations[\"name_len_diff\"] = all_annotations.apply(lambda x: abs(len(x[\"cert_name_stripped_version\"]) - len(x[\"referenced_cert_name_stripped_version\"])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ReferenceSegmentExtractor()(dset.certs.values())\n",
    "df = df.loc[df.label.notnull()].copy()\n",
    "df = df.merge(all_annotations.loc[:, [\"dgst\", \"referenced_cert_id\", \"name_similarity_stripped_version\", \"name_len_diff\", \"cert_name\", \"referenced_cert_name\"]], on=[\"dgst\", \"referenced_cert_id\"])\n",
    "\n",
    "# Simplified binary labels\n",
    "# label_mapping = {\"COMPONENT_USED\": \"COMPONENT_SHARED\", \"RECERTIFICATION\": \"PREVIOUS_VERSION\"}\n",
    "# df.label = df.label.map(lambda x: label_mapping[x] if x in label_mapping else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(segment: str, referenced_cert_id: str) -> str:\n",
    "    segment = segment.replace(referenced_cert_id, \"the referenced product\")\n",
    "    return segment\n",
    "\n",
    "df.segments = df.apply(lambda row: [process_segment(x, row.referenced_cert_id) for x in row.segments], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & evaluate the baseline classifier (majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier()\n",
    "dummy_clf.fit(df.loc[df.split == \"train\", [\"segments\"]], df.loc[df.split == \"train\"].label)\n",
    "y_pred_dummy = dummy_clf.predict(df.loc[df.split == \"valid\", [\"segments\"]])\n",
    "print(classification_report(df.loc[df.split == \"valid\"].label, y_pred_dummy, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & evaluate the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ReferenceAnnotatorTrainer.from_df(df, prec_recall_metric, mode=\"training\", use_analytical_rule_name_similarity=True, n_iterations=20, n_epochs=1, batch_size=16, segmenter_metric=\"f1\", ensemble_soft_voting_power=2)\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "annotator = trainer.clf\n",
    "df_predicted = annotator.predict_df(df)\n",
    "\n",
    "print(classification_report(df_predicted.loc[df_predicted.split == \"valid\", [\"y_pred\"]], df_predicted.loc[df_predicted.split == \"valid\", [\"label\"]], zero_division=0))\n",
    "\n",
    "# Print confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(df_predicted.loc[df_predicted.split == \"valid\", [\"label\"]], df_predicted.loc[df_predicted.split == \"valid\", [\"y_pred\"]], labels=list(trainer.label_mapping.values()), display_labels=list(trainer.label_mapping.values()), xticks_rotation=90)\n",
    "\n",
    "# Serialize errors into file\n",
    "df_predicted.y_proba = df_predicted.y_proba.map(lambda x: {y: z for y, z in zip(trainer.label_mapping.values(), x)})\n",
    "df_predicted.loc[~df_predicted.correct].to_json(\"/var/tmp/xjanovsk/certs/sec-certs/dataset/annotator_errors.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_trainer(trial, df):\n",
    "    use_analytical_rule_name_similarity = trial.suggest_categorical(\"use_analytical_rule_name_similarity\", [True, False])\n",
    "    n_iterations = trial.suggest_int(\"n_iterations\", 1, 50)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 1, 5)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 32)\n",
    "    segmenter_metric = trial.suggest_categorical(\"segmenter_metric\", [\"accuracy\", \"f1\"])\n",
    "    ensemble_soft_voting_power = trial.suggest_int(\"ensemble_soft_voting_power\", 1, 5)\n",
    "    return ReferenceAnnotatorTrainer.from_df(df, prec_recall_metric, mode=\"training\", use_analytical_rule_name_similarity=use_analytical_rule_name_similarity, n_iterations=n_iterations, n_epochs=n_epochs, batch_size=batch_size, segmenter_metric=segmenter_metric, ensemble_soft_voting_power=ensemble_soft_voting_power)\n",
    "\n",
    "def objective(trial):\n",
    "    trainer = define_trainer(trial, df)\n",
    "    trainer.train()\n",
    "    \n",
    "    annotator = trainer.clf\n",
    "    df_predicted = annotator.predict_df(df)\n",
    "    return f1_score(df_predicted.loc[df_predicted.split == \"valid\", [\"y_pred\"]], df_predicted.loc[df_predicted.split == \"valid\", [\"label\"]], zero_division=\"warn\", average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best Trial:\", best_trial.params)\n",
    "print(\"Best Trial Value:\", best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "ax.figure.savefig(\"/var/tmp/xjanovsk/certs/sec-certs/dataset/cc_refs_hyperparam_search/optimization_history.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "ax = optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "ax.figure.savefig(\"/var/tmp/xjanovsk/certs/sec-certs/dataset/cc_refs_hyperparam_search/param_importances.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "ax = optuna.visualization.matplotlib.plot_timeline(study)\n",
    "ax.figure.savefig(\"/var/tmp/xjanovsk/certs/sec-certs/dataset/cc_refs_hyperparam_search/timeline.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
