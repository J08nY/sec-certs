{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "\n",
    "# When on Aura, it is important to first set CUDA_VISIBLE_DEVICES environment variable directly from notebook\n",
    "# For available GPUs, see https://www.fi.muni.cz/tech/unix/aura.html.cs\n",
    "# Must be done before any related package that leverages cuda is imported\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-56c53afb-6f08-5e5b-83fa-32fc6f09eeb0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\n",
    "\n",
    "import pandas as pd\n",
    "from sec_certs.dataset import CCDataset\n",
    "from pathlib import Path\n",
    "from sec_certs.model.references.segment_extractor import ReferenceSegmentExtractor\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sec_certs.utils.helpers import compute_heuristics_version\n",
    "from rapidfuzz import fuzz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sec_certs.utils.nlp import softmax\n",
    "import torch\n",
    "\n",
    "\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "DATASET_PATH = REPO_ROOT / \"dataset/cc_final_run_may_23/dataset.json\"\n",
    "ANNOTATIONS_PATH = REPO_ROOT / \"src/sec_certs/data/reference_annotations/final/\"\n",
    "\n",
    "def replace_all(text: str, to_replce: set[str]) -> str:\n",
    "    for i in to_replce:\n",
    "        text = text.replace(i, \"\")\n",
    "    return text\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute name similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = pd.read_csv(ANNOTATIONS_PATH / \"train.csv\")\n",
    "valid_annotations = pd.read_csv(ANNOTATIONS_PATH / \"valid.csv\")\n",
    "all_annotations = pd.concat([train_annotations, valid_annotations])\n",
    "all_annotations = all_annotations[all_annotations.label != \"None\"].assign(label=lambda df: df.label.str.upper())\n",
    "\n",
    "dset = CCDataset.from_json(DATASET_PATH)\n",
    "all_certs = {x.dgst: x for x in dset.certs.values()}\n",
    "dset.certs = {x.dgst: x for x in dset.certs.values() if x.dgst in all_annotations.dgst.unique()}\n",
    "\n",
    "cert_id_to_name_mapping = {x.heuristics.cert_id: x.name for x in all_certs.values()}\n",
    "all_annotations[\"referenced_cert_name\"] = all_annotations[\"referenced_cert_id\"].map(cert_id_to_name_mapping)\n",
    "all_annotations[\"cert_name\"] = all_annotations[\"dgst\"].map(lambda x: dset[x].name)\n",
    "all_annotations[\"cert_versions\"] = all_annotations[\"cert_name\"].map(compute_heuristics_version)\n",
    "all_annotations = all_annotations.loc[all_annotations[\"referenced_cert_name\"].notnull()].copy()\n",
    "all_annotations[\"referenced_cert_versions\"] = all_annotations[\"referenced_cert_name\"].map(compute_heuristics_version)\n",
    "all_annotations[\"cert_name_stripped_version\"] = all_annotations.apply(lambda x: replace_all(x[\"cert_name\"], x[\"cert_versions\"]), axis=1)\n",
    "all_annotations[\"referenced_cert_name_stripped_version\"] = all_annotations.apply(lambda x: replace_all(x[\"referenced_cert_name\"], x[\"referenced_cert_versions\"]), axis=1)\n",
    "all_annotations[\"name_similarity\"] = all_annotations.apply(lambda x: fuzz.token_set_ratio(x[\"cert_name\"], x[\"referenced_cert_name\"]), axis=1)\n",
    "all_annotations[\"name_similarity_stripped_version\"] = all_annotations.apply(lambda x: fuzz.token_set_ratio(x[\"cert_name_stripped_version\"], x[\"referenced_cert_name_stripped_version\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ReferenceSegmentExtractor()(dset.certs.values())\n",
    "df = df.loc[df.label.notnull()].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF-IDF\n",
    "\n",
    "(Train on individual segments, then agregate the results with sum of probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "df_train_exploded = df.loc[df.split == \"train\"].explode(\"segments\")\n",
    "df_valid_exploded = df.loc[df.split == \"valid\"].explode(\"segments\")\n",
    "\n",
    "x_train = df_train_exploded[\"segments\"]\n",
    "y_train = df_train_exploded[\"label\"]\n",
    "\n",
    "x_valid = df_valid_exploded[\"segments\"]\n",
    "y_valid = df_valid_exploded[\"label\"]\n",
    "\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "x_valid_tfidf = vectorizer.transform(x_valid)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(x_train_tfidf, y_train)\n",
    "y_pred = clf.predict(x_valid_tfidf)\n",
    "\n",
    "df_train_exploded[\"y_proba\"] = clf.predict_proba(x_train_tfidf).tolist()\n",
    "df_valid_exploded[\"y_proba\"] = clf.predict_proba(x_valid_tfidf).tolist()\n",
    "\n",
    "# Get mapping of labels to indices of the clf random forest classifier\n",
    "label_mapping = {i: label for i, label in enumerate(clf.classes_)}\n",
    "\n",
    "# Now merge the segments back together and compute final prediction using the softmax of the sum of the probabilities\n",
    "df_valid_retrieved = df_valid_exploded.loc[:, [\"dgst\", \"referenced_cert_id\", \"y_proba\"]].groupby([\"dgst\", \"referenced_cert_id\"]).agg(list).reset_index()\n",
    "df_train_retrieved = df_train_exploded.loc[:, [\"dgst\", \"referenced_cert_id\", \"y_proba\"]].groupby([\"dgst\", \"referenced_cert_id\"]).agg(list).reset_index()\n",
    "\n",
    "def aggregate_results(x):\n",
    "    # Return the argmax of the sum of the probabilities obtained from the predictions on the individual segments\n",
    "    return label_mapping[int(np.argmax(softmax(np.power(x, 2).sum(axis=0))))]\n",
    "\n",
    "df_valid_retrieved[\"y_pred\"] = df_valid_retrieved[\"y_proba\"].map(aggregate_results)\n",
    "df_train_retrieved[\"y_pred\"] = df_train_retrieved[\"y_proba\"].map(aggregate_results)\n",
    "df_predictions = pd.concat([df_train_retrieved, df_valid_retrieved])\n",
    "df_final = df.merge(df_predictions, on=[\"dgst\", \"referenced_cert_id\"])\n",
    "\n",
    "# Finally print the classification report on the aggregated results\n",
    "print(classification_report(df_final.loc[df_final.split == \"valid\"].label, df_final.loc[df_final.split == \"valid\"].y_pred, zero_division=0))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(df_final.loc[df_final.split == \"valid\"].label, df_final.loc[df_final.split == \"valid\"].y_pred, xticks_rotation=90)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
