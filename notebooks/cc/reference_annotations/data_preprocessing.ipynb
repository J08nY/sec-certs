{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data for reference classification\n",
    "\n",
    "This script's pipeline is as follows:\n",
    "\n",
    "1. Recover text segments that surround certificate ID for all references in CC dataset\n",
    "2. Create a DataFrame `(dgst, cert_id, label, text_segments)` out of the objects\n",
    "3. Clean and dump into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from sec_certs.dataset import CCDataset\n",
    "from sec_certs.sample import CCCertificate\n",
    "import spacy\n",
    "from sec_certs.utils.parallel_processing import process_parallel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(\"../../../\").resolve()\n",
    "\n",
    "@dataclass\n",
    "class ReferenceRecord:\n",
    "    \"\"\"\n",
    "    Intermediate object to hold references for a given certificate together with sensible attributes to be extracted\n",
    "    for labeling.\n",
    "    \"\"\"\n",
    "    certificate: CCCertificate | None\n",
    "    dgst: str\n",
    "    cert_id: str\n",
    "    location: str\n",
    "    label: str | None = None\n",
    "    sentences: set[str] | None = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_sentences(doc, cert_id: str) -> set[str]:\n",
    "        \"\"\"\n",
    "        Return a set of sentences corresponding to the given cert_id for the record\n",
    "        \"\"\"\n",
    "        return {sent.text for sent in doc.sents if cert_id in sent.text}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cert_references_with_sentences(record: ReferenceRecord) -> set[tuple[str, str, str]]:\n",
    "        pth_to_read = (\n",
    "            record.certificate.state.st_txt_path\n",
    "            if record.location == \"target\"\n",
    "            else record.certificate.state.report_txt_path\n",
    "        )\n",
    "\n",
    "        with pth_to_read.open(\"r\") as handle:\n",
    "            data = handle.read()\n",
    "\n",
    "        result = ReferenceRecord.get_reference_sentences(nlp(data), record.cert_id)\n",
    "        record.sentences = result if result else None\n",
    "\n",
    "        return record\n",
    "\n",
    "    def to_pandas_tuple(self) -> tuple[str, str, str, str, set[str] | None]:\n",
    "        return self.dgst, self.cert_id, self.location, self.label, self.sentences\n",
    "\n",
    "def get_df_from_records(records: list[ReferenceRecord]):\n",
    "    \"\"\"\n",
    "    Builds dataframe with [dgst,cert_id,location,reason,sentences] with references from list of ReferenceRecords.\n",
    "    Reason set to None if not defined. \n",
    "    \"\"\"\n",
    "    results =  process_parallel(ReferenceRecord.get_cert_references_with_sentences, records, use_threading=False, progress_bar=True)\n",
    "    return pd.DataFrame.from_records([x.to_pandas_tuple() for x in results], columns=[\"dgst\", \"cert_id\", \"location\", \"label\", \"sentences\"])\n",
    "\n",
    "def preprocess_segment(segment):\n",
    "    segment = segment.replace(\"\\n\", \" \")\n",
    "    return segment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentences from text files and populate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:07<00:00,  8.26it/s]\n",
      "100%|██████████| 944/944 [01:08<00:00, 13.84it/s]\n",
      "100%|██████████| 2259/2259 [00:33<00:00, 68.33it/s]\n",
      "100%|██████████| 2551/2551 [00:02<00:00, 1089.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load annotated references from CSV\n",
    "annotations_df = pd.read_csv(REPO_ROOT / \"data/cert_id_eval/random_references.csv\")\n",
    "annotations_df = annotations_df.rename(columns={\"id\": \"dgst\", \"reason\": \"label\"})\n",
    "annotations_df = annotations_df.loc[annotations_df.label != \"self\"]\n",
    "annotations_df.label = annotations_df.label.map(lambda x: x.upper().replace(\" \", \"_\"))\n",
    "\n",
    "# Load dataset\n",
    "# dset = CCDataset.from_web_latest()\n",
    "dset = CCDataset.from_json(REPO_ROOT / \"datasets/cc/cc_dataset.json\")\n",
    "\n",
    "annotated_records = [\n",
    "    ReferenceRecord(dset[x.dgst], x.dgst, x.cert_id, x.location, x.label)\n",
    "    for x in annotations_df.itertuples(index=False)\n",
    "]\n",
    "\n",
    "# Reference records without annotations\n",
    "target_certs = [x for x in dset if x.heuristics.st_references.directly_referencing and x.state.st_txt_path]\n",
    "report_certs = [x for x in dset if x.heuristics.report_references.directly_referencing and x.state.report_txt_path]\n",
    "target_records = [\n",
    "    ReferenceRecord(x, x.dgst, y, \"target\", None, None)\n",
    "    for x in target_certs\n",
    "    for y in x.heuristics.st_references.directly_referencing\n",
    "]\n",
    "report_records = [\n",
    "    ReferenceRecord(x, x.dgst, y, \"report\", None, None)\n",
    "    for x in report_certs\n",
    "    for y in x.heuristics.report_references.directly_referencing\n",
    "]\n",
    "\n",
    "# Filter annotated_records from report_records to avoid duplicities\n",
    "annotated_keys = {(x.dgst, x.cert_id) for x in annotated_records}\n",
    "report_records = [x for x in report_records if (x.dgst, x.cert_id) not in annotated_keys]\n",
    "\n",
    "df_labeled = get_df_from_records(annotated_records)\n",
    "df_targets = get_df_from_records(target_records)\n",
    "df_reports = get_df_from_records(report_records)\n",
    "\n",
    "# Creates dictionary (dgst, cert_id): label to populate instances that have NULL label (on location==target) but were annotated in location==report and could adopt that label\n",
    "# This helps to avoid duplicities and extends the number of annotated sentences.\n",
    "dgst_cert_id_to_label_mapping = (\n",
    "    df_labeled.loc[df_labeled.label.notnull(), [\"dgst\", \"cert_id\", \"label\"]]\n",
    "    .drop_duplicates(subset=[\"dgst\", \"cert_id\"])\n",
    "    .set_index([\"dgst\", \"cert_id\"])\n",
    "    .label.to_dict()\n",
    ")\n",
    "\n",
    "df = pd.concat([df_labeled, df_targets, df_reports])\n",
    "\n",
    "# Check for label noise\n",
    "dgst_cert_id_tuples = (\n",
    "    df.drop_duplicates(subset=[\"dgst\", \"cert_id\"])\n",
    "    .loc[:, [\"dgst\", \"cert_id\"]]\n",
    "    .set_index([\"dgst\", \"cert_id\"])\n",
    "    .index.tolist()\n",
    ")\n",
    "duplicate_df = pd.DataFrame()\n",
    "for dgst, cert_id in tqdm(dgst_cert_id_tuples):\n",
    "    possible_duplicates = df.loc[(df.dgst == dgst) & (df.cert_id == cert_id) & (df.label.notnull())]\n",
    "    if (\n",
    "        possible_duplicates.shape[0] > 1\n",
    "        and not possible_duplicates.drop_duplicates(subset=[\"dgst\", \"cert_id\", \"label\"], keep=False).empty\n",
    "    ):\n",
    "        duplicate_df = pd.concat([duplicate_df, possible_duplicates])\n",
    "\n",
    "if not duplicate_df.empty:\n",
    "    print(\n",
    "        \"Warning, label noise detected, see `duplicate_df` for instances that have inconsistent label for `(dgst, cert_id)` key.\"\n",
    "    )\n",
    "\n",
    "# With no label noise, we should be safe to fill in labels for sentences found in targets such that the corresponding report was annotated\n",
    "df.label = df.copy().apply(\n",
    "    lambda row: dgst_cert_id_to_label_mapping.get(\n",
    "        (row[\"dgst\"], row[\"cert_id\"]) if pd.isnull(row[\"label\"]) else row[\"label\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataframes and dump to csv\n",
    "\n",
    "1. Version with `dgst, cert_id, location, single_sentence` as `*_exploded.csv`\n",
    "2. Version where all sentences tied to `(dgst, cert_id)` key are merged into `sentences`. Saved as `*_grouped.csv`\n",
    "\n",
    "*Note*: So far don't work with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split labels\n",
    "with (REPO_ROOT / \"data/reference_annotations_split/train.json\").open(\"r\") as handle:\n",
    "    train_digests = json.load(handle)\n",
    "\n",
    "with (REPO_ROOT / \"data/reference_annotations_split/valid.json\").open(\"r\") as handle:\n",
    "    valid_digests = json.load(handle)\n",
    "\n",
    "split_dct = {**dict.fromkeys(train_digests, \"train\"), **dict.fromkeys(valid_digests, \"valid\")}\n",
    "\n",
    "# Apply filtering\n",
    "# TODO: We should investigate the cases when we match no sentence\n",
    "df = df.loc[df.sentences.notnull()] \n",
    "df[\"split\"] = df.dgst.map(split_dct)\n",
    "df = df.loc[df.split.notnull()]  # Discard test samples\n",
    "\n",
    "# TODO: Add language detection\n",
    "\n",
    "# # Aggregate sentences from different sources (target, report) into one row\n",
    "df = df.groupby([\"dgst\", \"cert_id\", \"label\", \"split\"], as_index=False, dropna=False)[\"sentences\"].agg({\"sentences\": lambda x: set.union(*x)})\n",
    "df.to_csv(REPO_ROOT / \"datasets/reference_classification_dataset.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ed43df31f510d0b358bd0625493376557b0c4d37aa99c09b398809f951b6a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
