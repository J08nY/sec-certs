{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data for reference classification\n",
    "\n",
    "This script's pipeline is as follows:\n",
    "\n",
    "1. Recover text segments that surround certificate ID for all references in CC dataset\n",
    "2. Create a DataFrame `(dgst, cert_id, label, text_segments)` out of the objects\n",
    "3. Clean and dump into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from sec_certs.sample import CCCertificate\n",
    "from sec_certs.dataset import CCDataset\n",
    "import spacy\n",
    "from sec_certs.utils.parallel_processing import process_parallel\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(\"../../../\").resolve()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReferenceRecord:\n",
    "    \"\"\"\n",
    "    Intermediate object to hold references for a given certificate together with sensible attributes to be extracted\n",
    "    for labeling.\n",
    "    \"\"\"\n",
    "\n",
    "    certificate: CCCertificate | None\n",
    "    referenced_cert_id: str\n",
    "    source: str\n",
    "    label: str | None = None\n",
    "    sentences: set[str] | None = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_sentences(doc, referenced_cert_id: str) -> set[str]:\n",
    "        \"\"\"\n",
    "        Return a set of sentences corresponding to the given cert_id for the record\n",
    "        \"\"\"\n",
    "        return {sent.text for sent in doc.sents if referenced_cert_id in sent.text}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cert_references_with_sentences(record: ReferenceRecord) -> ReferenceRecord:\n",
    "        pth_to_read = (\n",
    "            record.certificate.state.st_txt_path\n",
    "            if record.source == \"target\"\n",
    "            else record.certificate.state.report_txt_path\n",
    "        )\n",
    "\n",
    "        with pth_to_read.open(\"r\") as handle:\n",
    "            data = handle.read()\n",
    "\n",
    "        result = ReferenceRecord.get_reference_sentences(nlp(data), record.referenced_cert_id)\n",
    "        record.sentences = result if result else None\n",
    "\n",
    "        return record\n",
    "\n",
    "    def to_pandas_tuple(self) -> tuple[str, str, str, str, set[str] | None]:\n",
    "        return self.certificate.dgst, self.referenced_cert_id, self.source, self.label, self.sentences\n",
    "\n",
    "\n",
    "def get_df_from_records(records: list[ReferenceRecord]):\n",
    "    \"\"\"\n",
    "    Builds dataframe with [dgst,cert_id,location,reason,sentences] with references from list of ReferenceRecords.\n",
    "    Reason set to None if not defined.\n",
    "    \"\"\"\n",
    "    results = process_parallel(\n",
    "        ReferenceRecord.get_cert_references_with_sentences, records, use_threading=False, progress_bar=True\n",
    "    )\n",
    "    return pd.DataFrame.from_records(\n",
    "        [x.to_pandas_tuple() for x in results], columns=[\"dgst\", \"referenced_cert_id\", \"source\", \"label\", \"sentences\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_segment(segment):\n",
    "    segment = segment.replace(\"\\n\", \" \")\n",
    "    return segment\n",
    "\n",
    "\n",
    "def get_split_dict(\n",
    "    train_path: Path | None = None, valid_path: Path | None = None, test_path: Path | None = None\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns dictionary that maps dgst: split, where split in `train`, `valid`, `test`. Expects path to list of dgsts for each split.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_single_dct(pth: Path | None, split_name: str) -> dict[str, str]:\n",
    "        if not pth:\n",
    "            return dict()\n",
    "        with pth.open(\"r\") as handle:\n",
    "            return dict.fromkeys(json.load(handle), split_name)\n",
    "\n",
    "    return {\n",
    "        **get_single_dct(train_path, \"train\"),\n",
    "        **get_single_dct(valid_path, \"valid\"),\n",
    "        **get_single_dct(test_path, \"test\"),\n",
    "    }\n",
    "\n",
    "def load_annotated_samples(\n",
    "    train_path: Path | None = None, valid_path: Path | None = None, test_path: Path | None = None\n",
    "):\n",
    "    def load_single_df(pth: Path | None, split_name: str) -> pd.DataFrame:\n",
    "        if not pth:\n",
    "            return pd.DataFrame()\n",
    "        return (\n",
    "            pd.read_csv(pth)\n",
    "            .assign(label=lambda df_: df_.label.str.replace(\" \", \"_\").str.upper(), split=split_name)\n",
    "            .replace(\"NONE\", None)\n",
    "            .dropna(subset=\"label\")\n",
    "        )\n",
    "\n",
    "    return pd.concat(\n",
    "        [load_single_df(train_path, \"train\"), load_single_df(valid_path, \"valid\"), load_single_df(test_path, \"test\")]\n",
    "    )[[\"dgst\", \"referenced_cert_id\", \"source\", \"label\", \"comment\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentences from text files and populate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 944/944 [01:05<00:00, 14.48it/s]\n",
      "100%|██████████| 2288/2288 [00:32<00:00, 69.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load annotated references from CSV\n",
    "annotations_df = load_annotated_samples(REPO_ROOT / \"data/reference_annotations/manual_annotations/train.csv\", REPO_ROOT / \"data/reference_annotations/manual_annotations/valid.csv\")\n",
    "\n",
    "# Load dataset\n",
    "# dset = CCDataset.from_web_latest()\n",
    "dset = CCDataset.from_json(REPO_ROOT / \"datasets/cc/cc_dataset.json\")\n",
    "\n",
    "target_certs = [x for x in dset if x.heuristics.st_references.directly_referencing and x.state.st_txt_path]\n",
    "report_certs = [x for x in dset if x.heuristics.report_references.directly_referencing and x.state.report_txt_path]\n",
    "target_records = [\n",
    "    ReferenceRecord(x, y, \"target\", None, None)\n",
    "    for x in target_certs\n",
    "    for y in x.heuristics.st_references.directly_referencing\n",
    "]\n",
    "report_records = [\n",
    "    ReferenceRecord(x, y, \"report\", None, None)\n",
    "    for x in report_certs\n",
    "    for y in x.heuristics.report_references.directly_referencing\n",
    "]\n",
    "\n",
    "# df_labeled = get_df_from_records(annotated_records)\n",
    "df_targets = get_df_from_records(target_records)\n",
    "df_reports = get_df_from_records(report_records)\n",
    "df = pd.concat([df_targets, df_reports])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataframes and dump to csv\n",
    "\n",
    "*Note*: So far don't work with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split labels\n",
    "split_dct = get_split_dict(\n",
    "    REPO_ROOT / \"data/reference_annotations/split/train.json\",\n",
    "    REPO_ROOT / \"data/reference_annotations/split/valid.json\",\n",
    "    REPO_ROOT / \"data/reference_annotations/split/test.json\",\n",
    ")\n",
    "\n",
    "# Creates dictionary `(dgst, cert_id): label`` to populate instances with manually assigned annotations.\n",
    "annotations_dict = (\n",
    "    annotations_df[[\"dgst\", \"referenced_cert_id\", \"label\"]].set_index([\"dgst\", \"referenced_cert_id\"]).label.to_dict()\n",
    ")\n",
    "\n",
    "# TODO: We should investigate the cases when we match no sentence, they may be new-lines and stuff\n",
    "# TODO: Add language detection\n",
    "# Process\n",
    "df = (\n",
    "    df.assign(\n",
    "        split=df.dgst.map(split_dct),\n",
    "        label=lambda df_: [annotations_dict.get(x) for x in zip(df_[\"dgst\"], df_[\"referenced_cert_id\"])],\n",
    "    )\n",
    "    .loc[lambda df_: (df_[\"sentences\"].notnull()) & (df_[\"split\"] != \"test\")]\n",
    "    .groupby([\"dgst\", \"referenced_cert_id\", \"label\", \"split\"], as_index=False, dropna=False)[\"sentences\"]\n",
    "    .agg({\"sentences\": lambda x: set.union(*x)})\n",
    ")\n",
    "df.to_csv(REPO_ROOT / \"datasets/reference_classification_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ed43df31f510d0b358bd0625493376557b0c4d37aa99c09b398809f951b6a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
