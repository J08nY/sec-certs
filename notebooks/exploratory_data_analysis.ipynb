{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "import os\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "def plot_feature_importances(model, x_test, y_test):\n",
    "    result = permutation_importance(model, x_test, y_test, n_repeats=20,\n",
    "                                random_state=42, n_jobs=8)\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.boxplot(result.importances[sorted_idx].T,\n",
    "               vert=False, labels=x_test.columns[sorted_idx])\n",
    "    ax.set_title(\"Permutation Importances (test set)\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "## Optional steps done once per new dataset version\n",
    "\n",
    "1. Load dataset from json to dataframe while extracting features\n",
    "2. Preprocess: Fill-in missing values, categorize numerical features\n",
    "3. Split the dataset into train and test parts, save them into respective files\n",
    "\n",
    "## Exploratory data analysis\n",
    "\n",
    "1. Load both train and test dataset from the files\n",
    "2. Do whatever you want. Don't forget that when looking at target variable, stay only on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pre-processing step (creates dataset)\n",
    "#import import_ipynb\n",
    "#import data_preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "train = pd.read_csv('/Users/adam/phd/projects/certificates/dataset/train.csv', index_col='index')\n",
    "test = pd.read_csv('/Users/adam/phd/projects/certificates/dataset/test.csv', index_col='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show histogram\n",
    "%matplotlib inline\n",
    "train.hist(bins=20, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlations between numerical features and the target feature\n",
    "train.corr()['sec_level_cat'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
