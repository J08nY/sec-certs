{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "    \n",
    "def split_dataset(df, features, label, dirpath, verbose=True, stratify=True):\n",
    "    feat_df = df.loc[:, features]\n",
    "    label_df = np.array(df.loc[:, label])\n",
    "    to_stratify = label_df if stratify is True else None\n",
    "    x_train, x_test, y_train, y_test = train_test_split(feat_df, label_df, test_size=0.2, random_state=42, stratify=to_stratify, shuffle=True)\n",
    "    \n",
    "    df_train = pd.DataFrame(x_train, columns=features, index=x_train.index)\n",
    "    df_train[label] = y_train\n",
    "    df_train.index.name = 'index'\n",
    "    \n",
    "    df_test = pd.DataFrame(x_test, columns=features, index=x_test.index)\n",
    "    df_test[label] = y_test\n",
    "    df_test.index.name = 'index'\n",
    "    \n",
    "    train_path = os.path.join(dirpath, 'train.csv')\n",
    "    test_path = os.path.join(dirpath, 'test.csv')\n",
    "    \n",
    "    df_train.to_csv(train_path)\n",
    "    df_test.to_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "with open('/Users/adam/phd/projects/certificates/dataset/certificate_data_complete_processed_analyzed.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create all features and fill a dataframe with them\n",
    "feature_dict = {x: [] for x in ['category', 'scheme', 'n_updates', 'sec_level_processed', 'cert_lab', 'n_pages',\n",
    "                           'cert_date', 'archived_date', 'manufacturer', 'protection_profiles', 'pdf_encrypted', 'defenses', 'crypto_algs']}\n",
    "cert_ids = list(data.keys())\n",
    "for key, val in data.items():\n",
    "    cert = data[key]\n",
    "    feature_dict['category'].append(cert['csv_scan'].get('cc_category', np.nan))\n",
    "    feature_dict['scheme'].append(cert['csv_scan'].get('cc_scheme', np.nan))\n",
    "    feature_dict['n_updates'].append(len(cert['csv_scan'].get('maintainance_updates', np.nan)))\n",
    "    feature_dict['sec_level_processed'].append(cert['processed'].get('cc_security_level', np.nan))\n",
    "    feature_dict['cert_lab'].append(cert['processed'].get('cert_lab', np.nan))\n",
    "    feature_dict['cert_date'].append(cert['csv_scan'].get('cc_certification_date', np.nan))\n",
    "    feature_dict['archived_date'].append(cert['csv_scan'].get('cc_archived_date', np.nan))\n",
    "    feature_dict['manufacturer'].append(cert['processed'].get('cc_manufacturer_simple', np.nan))\n",
    "    feature_dict['protection_profiles'].append(cert['csv_scan'].get('cc_protection_profiles', np.nan))\n",
    "\n",
    "    keywords_scan = cert.get('keywords_scan') or {}\n",
    "    feature_dict['defenses'].append(keywords_scan.get('rules_defenses', np.nan))\n",
    "    feature_dict['crypto_algs'].append(keywords_scan.get('rules_crypto_algs', np.nan))\n",
    "    \n",
    "    \n",
    "    meta_scan = cert.get('pdfmeta_scan') or {}\n",
    "    n_pages = meta_scan.get('pdf_number_of_pages', np.nan)\n",
    "    pdf_encrypted = meta_scan.get('pdf_is_encrypted', np.nan)\n",
    "    feature_dict['n_pages'].append(n_pages)\n",
    "    feature_dict['pdf_encrypted'].append(pdf_encrypted)\n",
    "df = pd.DataFrame(data=feature_dict, index=cert_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature description\n",
    "\n",
    "| Feature name | Feature type | N. unique val. | N. missing | notes |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| category | nominal | 15 | 0 | - |\n",
    "| scheme | nominal | 17 | 0 | 8 prevalent, merge below 100? |\n",
    "| n_updates | ordinal | 12 | 0 | set intervals [0,0), (1,1], (2,2], (3, inf) |\n",
    "| sec_level_processed | ordinal | 14 | 0 | + stands for additional defenses |\n",
    "| cert_lab | nominal | 13 | 2740 | Could be very useful, need to get more |\n",
    "| n_pages | numerical | - | 923 | Split into categories by histogram | \n",
    "| cert_date | Date | - | 0 | |\n",
    "| archived_date | Date | - | 0 | Beware of huge number of archived in Sep. 2019|\n",
    "| manufacturer | nominal | 807 | 10 | Need to merge if it should be of some use |\n",
    "| protection_profiles | nominal | 216 | 0 (or 2300)| Take length of list as feature |\n",
    "| pdf_encrypted | boolean | True/False | 674 | If missing set false |\n",
    "| defenses | nominal | - | 668 | To be processed to number of keys |\n",
    "| crypto_algs | nominal | - | 668 | To be processed to number of algs. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check null values and fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pdf_encrypted.fillna(False, inplace=True)\n",
    "df.defenses = df.defenses.map(lambda x: {} if pd.isnull(x) else x)\n",
    "df.crypto_algs = df.crypto_algs.map(lambda x: {} if pd.isnull(x) else x)\n",
    "\n",
    "n_pages_median = df.n_pages.median()\n",
    "df.n_pages.fillna(n_pages_median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize nominal and ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map defenses and crypto algs to the length of the list\n",
    "map_to_len = lambda x: len(x) if isinstance(x, dict) else 0\n",
    "df['n_defenses'] = df.defenses.map(map_to_len)\n",
    "df['n_crypto_algs'] = df.crypto_algs.map(map_to_len)\n",
    "\n",
    "# map protection profiles to the length of the list\n",
    "prot_prof_to_len = lambda x: len(x.split(',')) if x != '' else 0\n",
    "df['n_protection_profiles'] = df.protection_profiles.map(prot_prof_to_len)\n",
    "\n",
    "# Map security level into ordered categories\n",
    "sec_level_dict = {'EAL1': 0, 'EAL1+': 1, 'EAL2': 2, 'EAL2+': 3, 'EAL3': 4, 'EAL3+': 5, 'EAL4': 6, 'EAL4+': 7, 'EAL5': 8, 'EAL5+': 9, 'EAL6': 10, 'EAL6+': 11, 'EAL7': 12, 'EAL7+': 13}\n",
    "df['sec_level_cat'] = df.sec_level_processed.map(sec_level_dict)\n",
    "sec_level_median = df['sec_level_cat'].loc[df.sec_level_cat != -1].median()\n",
    "df['sec_level_cat'].fillna(sec_level_median, inplace=True)\n",
    "\n",
    "# categorize n_pages into intervals (ordered)\n",
    "df['n_pages_cat'] = pd.cut(df.n_pages, [0, 10, 15, 30, 40, 50, 5000], labels=[x for x in range(6)])\n",
    "df.n_pages_cat = df['n_pages_cat'].astype('int64')\n",
    "\n",
    "# Categories number of updates to intervals\n",
    "df['n_updates_cat'] = pd.cut(df.n_updates, [-1, 0, 1, 2, 3, 100], labels=[x for x in range(5)])\n",
    "df['n_updates_cat'] = df['n_updates_cat'].astype('int64')\n",
    "\n",
    "# Create year of certification feature\n",
    "df.cert_date = pd.to_datetime(df.cert_date)\n",
    "df.archived_date = pd.to_datetime(df.archived_date)\n",
    "df['cert_year'] = df.cert_date.dt.year\n",
    "\n",
    "# Categorize schemes, not sure if wise\n",
    "n_occurences = df.scheme.value_counts()\n",
    "schemes_to_merge = list(n_occurences.loc[n_occurences < 100].index)\n",
    "df['scheme_cat'] = df.scheme.map(lambda x: x if x not in schemes_to_merge else 'Other')\n",
    "\n",
    "df.category = df.category.astype('category').cat.codes\n",
    "df.scheme_cat = df.scheme_cat.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset(df, ['category', 'n_updates', 'pdf_encrypted', 'n_defenses', 'n_crypto_algs', 'n_protection_profiles',\n",
    "                   'n_pages_cat', 'n_updates_cat', 'cert_year', 'scheme_cat'],\n",
    "                   'sec_level_cat',\n",
    "                   '/Users/adam/phd/projects/certificates/dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
